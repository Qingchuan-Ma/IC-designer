# 人工智能芯片设计

## 时域
### GPGPU
## 空域
### 数据复用
### 映射方法
### 存储优化

## 模型压缩

深度学习已经在实验室条件下展现出对各种任务的出色应对能力。然而由于高昂的计算代价，部署网络到移动端存在困难。对于目标检测任务，深层神经网络对硬件的内存和计算能力要求高，若还有实时性要求，则更难在资源受限的终端部署模型。

为了解决这个问题，前人已有许多关于模型压缩的研究。模型压缩希望在不过多损失准确度的条件下，给模型“瘦身”，降低模型推理时的资源需求，使模型更易部署。

本文分别梳理模型压缩的三种技术:模型剪枝、模型量化和知识蒸馏。对每一种技术，都介绍原理和分类及常用方法。

本文的主要内容有：

1.对模型压缩技术研究现状和各压缩技术之间的关系进行概述。

2.介绍剪枝技术。说明剪枝原理和分类方式，又分别从基于性质重要性、适应重要性和重构的剪枝这三种标准说明剪枝方法，随后对剪枝流程进行总结，接着反思剪枝工作的意义。

3.介绍量化技术。说明量化的基本原理和分类方式，之后介绍经典量化模型，包括Google whitepaper量化方法，DorefaNet量化方法和TensorRT量化方法，然后说明量化部署到硬件上的实现细节。

4.介绍知识蒸馏的工作原理和分类 。